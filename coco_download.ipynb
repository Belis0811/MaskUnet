{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading panoptic_annotations_trainval2017.zip: 100%|██████████| 821M/821M [01:15<00:00, 11.4MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset downloaded and extracted:\n",
      "Annotations: ./COCO/annotations\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import zipfile\n",
    "\n",
    "# URLs for the required files\n",
    "urls = {\n",
    "    # \"instances_train2017.json\": \"http://images.cocodataset.org/annotations/annotations_trainval2017.zip\",\n",
    "    # \"instances_val2017.json\": \"http://images.cocodataset.org/annotations/annotations_trainval2017.zip\",\n",
    "    # \"train2017\": \"http://images.cocodataset.org/zips/train2017.zip\",\n",
    "    # \"val2017\": \"http://images.cocodataset.org/zips/val2017.zip\",\n",
    "    \"panoptic_train2017.json\": \"http://images.cocodataset.org/annotations/panoptic_annotations_trainval2017.zip\",\n",
    "    \"panoptic_val2017.json\": \"http://images.cocodataset.org/annotations/panoptic_annotations_trainval2017.zip\",\n",
    "}\n",
    "\n",
    "# Directory to store the dataset\n",
    "output_dir = \"./COCO\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "def download_file(url, output_path):\n",
    "    \"\"\"Downloads a file with a progress bar.\"\"\"\n",
    "    response = requests.get(url, stream=True)\n",
    "    total_size = int(response.headers.get('content-length', 0))\n",
    "    with open(output_path, \"wb\") as file, tqdm(\n",
    "        desc=f\"Downloading {os.path.basename(output_path)}\",\n",
    "        total=total_size,\n",
    "        unit=\"B\",\n",
    "        unit_scale=True,\n",
    "        unit_divisor=1024,\n",
    "    ) as bar:\n",
    "        for data in response.iter_content(chunk_size=1024):\n",
    "            file.write(data)\n",
    "            bar.update(len(data))\n",
    "\n",
    "def extract_file(zip_path, extract_to, specific_files=None):\n",
    "    \"\"\"Extracts specific files or all files from a zip archive.\"\"\"\n",
    "    with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
    "        if specific_files:\n",
    "            for file in specific_files:\n",
    "                if file in zip_ref.namelist():\n",
    "                    zip_ref.extract(file, extract_to)\n",
    "        else:\n",
    "            zip_ref.extractall(extract_to)\n",
    "\n",
    "# Step 1: Download and Extract Annotations\n",
    "annotations_zip_path = os.path.join(output_dir, \"panoptic_annotations_trainval2017.zip\")\n",
    "if not os.path.exists(annotations_zip_path):\n",
    "    download_file(urls[\"panoptic_train2017.json\"], annotations_zip_path)\n",
    "\n",
    "extract_file(\n",
    "    annotations_zip_path,\n",
    "    output_dir,\n",
    "    specific_files=[\"annotations/panoptic_train2017.json\", \"annotations/panoptic_val2017.json\"],\n",
    ")\n",
    "\n",
    "# Step 2: Download and Extract Images (Train)\n",
    "# train_zip_path = os.path.join(output_dir, \"train2017.zip\")\n",
    "# if not os.path.exists(train_zip_path):\n",
    "#     download_file(urls[\"train2017\"], train_zip_path)\n",
    "\n",
    "# extract_file(train_zip_path, output_dir)\n",
    "\n",
    "# Step 3: Download and Extract Images (Validation)\n",
    "# val_zip_path = os.path.join(output_dir, \"val2017.zip\")\n",
    "# if not os.path.exists(val_zip_path):\n",
    "#     download_file(urls[\"val2017\"], val_zip_path)\n",
    "\n",
    "# extract_file(val_zip_path, output_dir)\n",
    "\n",
    "# Final output paths\n",
    "annotations_dir = os.path.join(output_dir, \"annotations\")\n",
    "# train_images_dir = os.path.join(output_dir, \"train2017\")\n",
    "# val_images_dir = os.path.join(output_dir, \"val2017\")\n",
    "\n",
    "print(\"Dataset downloaded and extracted:\")\n",
    "print(f\"Annotations: {annotations_dir}\")\n",
    "# print(f\"Train images: {train_images_dir}\")\n",
    "# print(f\"Validation images: {val_images_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=13.22s)\n",
      "creating index...\n",
      "index created!\n",
      "Filtered dataset saved to filtered_instances.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pycocotools.coco import COCO\n",
    "\n",
    "# Define the path to the original COCO dataset annotation file\n",
    "input_json_path = './COCO/annotations/instances_train2017.json'\n",
    "output_json_path = 'filtered_instances.json'\n",
    "\n",
    "# Define the category IDs to keep (example IDs for things and stuff)\n",
    "things_ids = [1, 2, 3, ..., 80]  # Replace with actual IDs for things\n",
    "stuff_ids = [81, 82, 83, ..., 133]  # Replace with actual IDs for stuff\n",
    "category_ids_to_keep = set(things_ids + stuff_ids)\n",
    "\n",
    "# Load the original COCO dataset\n",
    "coco = COCO(input_json_path)\n",
    "\n",
    "# Filter annotations\n",
    "filtered_annotations = []\n",
    "for annotation in coco.dataset['annotations']:\n",
    "    if annotation['category_id'] in category_ids_to_keep:\n",
    "        filtered_annotations.append(annotation)\n",
    "\n",
    "# Filter images\n",
    "image_ids_to_keep = {ann['image_id'] for ann in filtered_annotations}\n",
    "filtered_images = [img for img in coco.dataset['images'] if img['id'] in image_ids_to_keep]\n",
    "\n",
    "# Filter categories\n",
    "filtered_categories = [cat for cat in coco.dataset['categories'] if cat['id'] in category_ids_to_keep]\n",
    "\n",
    "# Save the filtered dataset\n",
    "filtered_data = {\n",
    "    'images': filtered_images,\n",
    "    'annotations': filtered_annotations,\n",
    "    'categories': filtered_categories\n",
    "}\n",
    "with open(output_json_path, 'w') as f:\n",
    "    json.dump(filtered_data, f)\n",
    "\n",
    "print(f\"Filtered dataset saved to {output_json_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anzhe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
