{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading panoptic_annotations_trainval2017.zip: 100%|██████████| 821M/821M [01:05<00:00, 13.1MB/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset downloaded and extracted:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'annotations_dir' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 76\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# extract_file(\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m#     annotations_zip_path,\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m#     output_dir,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m# train_images_dir = os.path.join(output_dir, \"train2017\")\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# val_images_dir = os.path.join(output_dir, \"val2017\")\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset downloaded and extracted:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 76\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnnotations: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mannotations_dir\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;66;03m# print(f\"Train images: {train_images_dir}\")\u001b[39;00m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m# print(f\"Validation images: {val_images_dir}\")\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'annotations_dir' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import zipfile\n",
    "\n",
    "# URLs for the required files\n",
    "urls = {\n",
    "    # \"instances_train2017.json\": \"http://images.cocodataset.org/annotations/annotations_trainval2017.zip\",\n",
    "    # \"instances_val2017.json\": \"http://images.cocodataset.org/annotations/annotations_trainval2017.zip\",\n",
    "    \"train2017\": \"http://images.cocodataset.org/zips/train2017.zip\",\n",
    "    \"val2017\": \"http://images.cocodataset.org/zips/val2017.zip\",\n",
    "    \"panoptic_train2017.json\": \"http://images.cocodataset.org/annotations/panoptic_annotations_trainval2017.zip\",\n",
    "    \"panoptic_val2017.json\": \"http://images.cocodataset.org/annotations/panoptic_annotations_trainval2017.zip\",\n",
    "}\n",
    "\n",
    "# Directory to store the dataset\n",
    "output_dir = \"./COCO\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "def download_file(url, output_path):\n",
    "    \"\"\"Downloads a file with a progress bar.\"\"\"\n",
    "    response = requests.get(url, stream=True)\n",
    "    total_size = int(response.headers.get('content-length', 0))\n",
    "    with open(output_path, \"wb\") as file, tqdm(\n",
    "        desc=f\"Downloading {os.path.basename(output_path)}\",\n",
    "        total=total_size,\n",
    "        unit=\"B\",\n",
    "        unit_scale=True,\n",
    "        unit_divisor=1024,\n",
    "    ) as bar:\n",
    "        for data in response.iter_content(chunk_size=1024):\n",
    "            file.write(data)\n",
    "            bar.update(len(data))\n",
    "\n",
    "def extract_file(zip_path, extract_to, specific_files=None):\n",
    "    \"\"\"Extracts specific files or all files from a zip archive.\"\"\"\n",
    "    with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
    "        if specific_files:\n",
    "            for file in specific_files:\n",
    "                if file in zip_ref.namelist():\n",
    "                    zip_ref.extract(file, extract_to)\n",
    "        else:\n",
    "            zip_ref.extractall(extract_to)\n",
    "\n",
    "# Step 1: Download and Extract Annotations\n",
    "annotations_zip_path = os.path.join(output_dir, \"panoptic_annotations_trainval2017.zip\")\n",
    "if not os.path.exists(annotations_zip_path):\n",
    "    download_file(urls[\"panoptic_train2017.json\"], annotations_zip_path)\n",
    "\n",
    "# extract_file(\n",
    "#     annotations_zip_path,\n",
    "#     output_dir,\n",
    "#     specific_files=[\"annotations/panoptic_train2017.json\", \"annotations/panoptic_val2017.json\"],\n",
    "# )\n",
    "\n",
    "# # Step 2: Download and Extract Images (Train)\n",
    "# train_zip_path = os.path.join(output_dir, \"train2017.zip\")\n",
    "# # if not os.path.exists(train_zip_path):\n",
    "# #     download_file(urls[\"train2017\"], train_zip_path)\n",
    "\n",
    "# extract_file(train_zip_path, output_dir)\n",
    "\n",
    "# # Step 3: Download and Extract Images (Validation)\n",
    "# val_zip_path = os.path.join(output_dir, \"val2017.zip\")\n",
    "# if not os.path.exists(val_zip_path):\n",
    "#     download_file(urls[\"val2017\"], val_zip_path)\n",
    "\n",
    "# extract_file(val_zip_path, output_dir)\n",
    "\n",
    "# Final output paths\n",
    "# annotations_dir = os.path.join(output_dir, \"annotations\")\n",
    "# train_images_dir = os.path.join(output_dir, \"train2017\")\n",
    "# val_images_dir = os.path.join(output_dir, \"val2017\")\n",
    "\n",
    "print(\"Dataset downloaded and extracted:\")\n",
    "print(f\"Annotations: {annotations_dir}\")\n",
    "# print(f\"Train images: {train_images_dir}\")\n",
    "# print(f\"Validation images: {val_images_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset dataset created with 10000 samples.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "from shutil import copy2\n",
    "\n",
    "TRAIN_PATH = './COCO/'\n",
    "# Paths\n",
    "original_json_path = os.path.join(TRAIN_PATH, 'annotations', 'panoptic_train2017.json')\n",
    "subset_json_path = os.path.join(TRAIN_PATH, 'annotations', 'panoptic_train_subset.json')\n",
    "original_img_dir = os.path.join(TRAIN_PATH, 'train2017')\n",
    "subset_img_dir = os.path.join(TRAIN_PATH, 'train_subset')\n",
    "subset_panoptic_dir = os.path.join(TRAIN_PATH, 'panoptic_train_subset')\n",
    "\n",
    "# Parameters\n",
    "num_samples = 10000\n",
    "\n",
    "# Create directories for subset\n",
    "os.makedirs(subset_img_dir, exist_ok=True)\n",
    "os.makedirs(subset_panoptic_dir, exist_ok=True)\n",
    "\n",
    "# Load original JSON\n",
    "with open(original_json_path, 'r') as f:\n",
    "    panoptic_data = json.load(f)\n",
    "\n",
    "# Randomly sample images\n",
    "sampled_images = random.sample(panoptic_data['images'], num_samples)\n",
    "sampled_image_ids = {img['id'] for img in sampled_images}\n",
    "\n",
    "# Filter annotations for sampled images\n",
    "sampled_annotations = [ann for ann in panoptic_data['annotations'] if ann['image_id'] in sampled_image_ids]\n",
    "\n",
    "# Copy sampled images and annotations\n",
    "for img in sampled_images:\n",
    "    src_img_path = os.path.join(original_img_dir, img['file_name'])\n",
    "    dest_img_path = os.path.join(subset_img_dir, img['file_name'])\n",
    "    copy2(src_img_path, dest_img_path)\n",
    "\n",
    "for ann in sampled_annotations:\n",
    "    src_ann_path = os.path.join(TRAIN_PATH, 'panoptic_train2017','panoptic_train2017', ann['file_name'])\n",
    "    dest_ann_path = os.path.join(subset_panoptic_dir, ann['file_name'])\n",
    "    copy2(src_ann_path, dest_ann_path)\n",
    "\n",
    "# Save the new JSON\n",
    "subset_data = {\n",
    "    'images': sampled_images,\n",
    "    'annotations': sampled_annotations,\n",
    "    'categories': panoptic_data['categories'],\n",
    "}\n",
    "with open(subset_json_path, 'w') as f:\n",
    "    json.dump(subset_data, f)\n",
    "\n",
    "print(f\"Subset dataset created with {num_samples} samples.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anzhe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
